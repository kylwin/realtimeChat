import { useState, useCallback, useRef, useEffect } from 'react'
import type { Message, ConnectionStatus, ConversationState, CheckReservationResponse } from '@/types'

interface UseRealtimeChatOptions {
  apiKey?: string
  webhookUrl?: string
  onMessage?: (message: Message) => void
  onError?: (error: Error) => void
}

// Constants
const TEMP_USER_ID = 'temp-user-transcript'
const TEMP_ASSISTANT_ID = 'temp-assistant-transcript'
const OPENAI_MODEL = 'gpt-4o-realtime-preview-2024-12-17'
const DATA_CHANNEL_NAME = 'oai-events'
const CHECK_RESV_URL = 'https://ici.zeabur.app/webhook/checkResv'

// Helper: Extract client secret from various response formats
function extractClientSecret(data: string): string | null {
  // Try JSON parsing first
  try {
    const jsonData = JSON.parse(data)

    // Array format (full OpenAI response from N8N)
    if (Array.isArray(jsonData) && jsonData.length > 0) {
      return jsonData[0]?.client_secret?.value || null
    }

    // Object format with nested value
    if (jsonData.client_secret) {
      if (typeof jsonData.client_secret === 'string') {
        return jsonData.client_secret
      }
      if (jsonData.client_secret.value) {
        return jsonData.client_secret.value
      }
    }

    // Alternative property names
    return jsonData.clientSecret || jsonData.data?.client_secret || null
  } catch {
    // Not JSON, try regex extraction from HTML
    const patterns = [
      /EPHEMERAL_KEY\s*=\s*["']([^"']+)["']/,
      /clientSecret\s*=\s*["']([^"']+)["']/,
      /client_secret["']?\s*:\s*["']([^"']+)["']/,
      /client_secret["']?\s*=\s*["']([^"']+)["']/
    ]

    for (const pattern of patterns) {
      const match = data.match(pattern)
      if (match?.[1]) return match[1]
    }

    return null
  }
}

// Helper: Update or create temporary message
function updateOrCreateTempMessage(
  messages: Message[],
  tempId: string,
  role: 'user' | 'assistant',
  content: string,
  timestamp: number,
  insertAfterIndex?: number
): Message[] {
  const existingIndex = messages.findIndex(m => m.id === tempId)

  if (existingIndex !== -1) {
    // Update existing temp message
    const updated = [...messages]
    updated[existingIndex] = {
      ...updated[existingIndex],
      content,
      timestamp
    }
    return updated
  }

  // Create new temp message
  const newMessage: Message = {
    id: tempId,
    role,
    content,
    timestamp
  }

  // Insert at specific position or add to end
  if (insertAfterIndex !== undefined && insertAfterIndex !== -1) {
    const updated = [...messages]
    updated.splice(insertAfterIndex + 1, 0, newMessage)
    return updated
  }

  return [...messages, newMessage]
}

// Helper: Replace temp message with final one
function replaceTempMessage(
  messages: Message[],
  tempId: string,
  finalMessage: Message,
  ensureBeforeTempId?: string
): Message[] {
  const tempIndex = messages.findIndex(m => m.id === tempId)
  const otherTempIndex = ensureBeforeTempId
    ? messages.findIndex(m => m.id === ensureBeforeTempId)
    : -1

  if (tempIndex !== -1) {
    const updated = [...messages]
    updated[tempIndex] = finalMessage

    // Ensure correct ordering if needed
    if (otherTempIndex !== -1 && otherTempIndex < tempIndex) {
      const otherMsg = updated[otherTempIndex]
      updated.splice(otherTempIndex, 1)
      const newIndex = updated.findIndex(m => m.id === finalMessage.id)
      updated.splice(newIndex + 1, 0, otherMsg)
    }

    return updated
  }

  // No temp message found - insert before other temp if exists
  if (otherTempIndex !== -1) {
    const updated = [...messages]
    updated.splice(otherTempIndex, 0, finalMessage)
    return updated
  }

  // Otherwise add to end
  return [...messages, finalMessage]
}

// Helper: Extract time from AI's response
function extractTimeFromAIResponse(text: string): string | null {
  console.log('üïê Extracting time from AI response:', text)

  // Special cases first
  if (text.includes('‰∏≠Âçà')) {
    console.log('‚úÖ Detected: ‰∏≠Âçà ‚Üí 12:00')
    return '12:00'
  }

  // Pattern: "Ë´ãÁ®çÁ≠âÔºåÊàëÂπ´‰Ω†Êü•‰∏Ä‰∏ã [ÊôÇÈñì]"
  const patterns = [
    // ‰∏≠ÊñáÊôÇÈñìÊ†ºÂºè - Ê®ôÊ∫ñÈªûÊï∏
    { regex: /Êü•‰∏Ä‰∏ã\s*(\d{1,2})\s*Èªû\s*Âçä/, handler: (m: RegExpMatchArray) => {
      const hour = parseInt(m[1])
      return `${hour.toString().padStart(2, '0')}:30`
    }},
    { regex: /Êü•‰∏Ä‰∏ã\s*(\d{1,2})\s*Èªû\s*(\d{1,2})\s*ÂàÜ/, handler: (m: RegExpMatchArray) => {
      const hour = parseInt(m[1])
      const minute = parseInt(m[2])
      return `${hour.toString().padStart(2, '0')}:${minute.toString().padStart(2, '0')}`
    }},
    { regex: /Êü•‰∏Ä‰∏ã\s*(\d{1,2})\s*Èªû(?!\s*Âçä|\s*\d)/, handler: (m: RegExpMatchArray) => {
      const hour = parseInt(m[1])
      return `${hour.toString().padStart(2, '0')}:00`
    }},

    // Êó©‰∏ä/‰∏ãÂçà/Êôö‰∏ä
    { regex: /Êü•‰∏Ä‰∏ã\s*Êó©‰∏ä\s*(\d{1,2})\s*Èªû/, handler: (m: RegExpMatchArray) => {
      let hour = parseInt(m[1])
      if (hour === 12) hour = 0
      return `${hour.toString().padStart(2, '0')}:00`
    }},
    { regex: /Êü•‰∏Ä‰∏ã\s*‰∏ãÂçà\s*(\d{1,2})\s*Èªû/, handler: (m: RegExpMatchArray) => {
      let hour = parseInt(m[1])
      if (hour < 12) hour += 12
      return `${hour.toString().padStart(2, '0')}:00`
    }},
    { regex: /Êü•‰∏Ä‰∏ã\s*Êôö‰∏ä\s*(\d{1,2})\s*Èªû/, handler: (m: RegExpMatchArray) => {
      let hour = parseInt(m[1])
      if (hour < 12) hour += 12
      return `${hour.toString().padStart(2, '0')}:00`
    }},

    // 24Â∞èÊôÇÂà∂Ê†ºÂºè
    { regex: /Êü•‰∏Ä‰∏ã\s*(\d{1,2}):(\d{2})/, handler: (m: RegExpMatchArray) => {
      const hour = parseInt(m[1])
      const minute = parseInt(m[2])
      return `${hour.toString().padStart(2, '0')}:${minute.toString().padStart(2, '0')}`
    }},

    // PM/AM Ê†ºÂºè
    { regex: /Êü•‰∏Ä‰∏ã\s*(\d{1,2})\s*pm/i, handler: (m: RegExpMatchArray) => {
      let hour = parseInt(m[1])
      if (hour < 12) hour += 12
      return `${hour.toString().padStart(2, '0')}:00`
    }},
    { regex: /Êü•‰∏Ä‰∏ã\s*(\d{1,2})\s*am/i, handler: (m: RegExpMatchArray) => {
      let hour = parseInt(m[1])
      if (hour === 12) hour = 0
      return `${hour.toString().padStart(2, '0')}:00`
    }},
  ]

  // Try each pattern
  for (const { regex, handler } of patterns) {
    const match = text.match(regex)
    if (match) {
      try {
        const timeStr = handler(match)
        console.log(`‚úÖ Extracted time: ${timeStr} from "${text}"`)
        return timeStr
      } catch (error) {
        console.error('Error in time extraction handler:', error)
        continue
      }
    }
  }

  console.log('‚ùå Could not extract time from AI response')
  return null
}

// Helper: Call reservation check API
async function checkReservationAvailability(time: string): Promise<CheckReservationResponse> {
  console.log('üåê Calling reservation API:', {
    url: CHECK_RESV_URL,
    time,
    timestamp: new Date().toISOString()
  })

  try {
    const response = await fetch(CHECK_RESV_URL, {
      method: 'POST',
      mode: 'cors',
      headers: {
        'Content-Type': 'application/json',
        'Accept': 'application/json'
      },
      body: JSON.stringify({ time })
    })

    console.log('üì° API response received:', {
      status: response.status,
      statusText: response.statusText,
      ok: response.ok,
      headers: {
        'content-type': response.headers.get('content-type'),
        'access-control-allow-origin': response.headers.get('access-control-allow-origin')
      }
    })

    if (!response.ok) {
      const errorText = await response.text()
      console.error('‚ùå API error response:', errorText)
      throw new Error(`API request failed: ${response.status} ${response.statusText}`)
    }

    const data = await response.json()

    console.log('üì• Raw API response:', data)

    // Handle response format: { bookTime: "12:00", availability: true/false }
    // Support multiple field name variations
    const availabilityValue = data.Availability ?? data.availability ?? data.available ?? false

    const result = {
      available: availabilityValue,
      time: data.bookTime ?? time,
      message: data.message
    }

    console.log('üì¶ Parsed result:', {
      result,
      availabilityValue,
      'data.Availability': data.Availability,
      'data.availability': data.availability,
      'data.available': data.available
    })

    return result
  } catch (error) {
    console.error('‚ùå Error checking reservation:', {
      error,
      errorName: error instanceof Error ? error.name : 'Unknown',
      errorMessage: error instanceof Error ? error.message : String(error),
      errorStack: error instanceof Error ? error.stack : undefined
    })
    throw error
  }
}

export function useRealtimeChat(options: UseRealtimeChatOptions = {}) {
  const [messages, setMessages] = useState<Message[]>([])
  const [connectionStatus, setConnectionStatus] = useState<ConnectionStatus>('disconnected')
  const [conversationState, setConversationState] = useState<ConversationState>('idle')
  const [isAudioEnabled, setIsAudioEnabled] = useState(false)

  const peerConnectionRef = useRef<RTCPeerConnection | null>(null)
  const dataChannelRef = useRef<RTCDataChannel | null>(null)
  const audioElementRef = useRef<HTMLAudioElement | null>(null)
  const mediaStreamRef = useRef<MediaStream | null>(null)
  const currentTranscriptRef = useRef({ user: '', assistant: '' })
  const userMessageTimestampRef = useRef<number | null>(null)
  const assistantMessageTimestampRef = useRef<number | null>(null)
  const actionProcessingRef = useRef<boolean>(false)

  const webhookUrl = options.webhookUrl ||
    import.meta.env.VITE_WEBHOOK_URL ||
    'https://ici.zeabur.app/webhook/realtime-ai'

  // Handle data channel messages
  const handleDataChannelMessage = useCallback(
    async (message: any) => {
      const { type } = message
  
      // ---- Â∞èÂ∑•ÂÖ∑ÔºöÂ∞ÅË£ù‰∏ÄÊ¨°ËôïÁêÜ CHECK_AVAILABILITY ÁöÑÊµÅÁ®ã ----
      const processCheckAvailability = async (time: string) => {
        try {
          console.log('üîç Checking availability for time:', time)
          const result = await checkReservationAvailability(time)
          console.log('‚úÖ Availability check result:', result)
  
          if (dataChannelRef.current?.readyState === 'open') {
            const payload = {
              bookTime: result.time,
              Availability: result.available, // false=Êúâ‰Ωç, true=Ê≤í‰ΩçÔºàÊåâ prompts ÂÆöÁæ©Ôºâ
            }
            const text = `AVAILABILITY_RESULT: ${JSON.stringify(payload)}`
  
            console.log('üì§ Sending availability result to Realtime AI:', {
              payload,
              text,
              fullResult: result,
            })
  
            const event = {
              type: 'conversation.item.create',
              item: {
                type: 'message',
                role: 'user',
                content: [{ type: 'input_text', text }],
              },
            }
  
            // ÊääÊü•Ë°®ÁµêÊûú‰∏üÁµ¶ Realtime Ê®°Âûã
            dataChannelRef.current.send(JSON.stringify(event))
  
            // ‚≠ê Êü•Ë°®ÂÆåÊàê ‚Üí Ëß£Èô§ÈùúÈü≥ÔºàËÆì„Äå‰∏ÉÈªûÊúâÁ©∫‰Ωç„ÄçÈÄôÂè•ÂèØ‰ª•Ë¢´Ë™™Âá∫‰æÜÔºâ
            if (audioElementRef.current) {
              audioElementRef.current.muted = false
            }
  
            // Ëß∏ÁôºÊñ∞ÁöÑÂõûË¶Ü
            dataChannelRef.current.send(JSON.stringify({ type: 'response.create' }))
          }
        } catch (error) {
          console.error('‚ùå Error processing CHECK_AVAILABILITY action:', error)
  
          if (dataChannelRef.current?.readyState === 'open') {
            const errorText = 'AVAILABILITY_RESULT: {"error": "Êü•Ë©¢Â§±ÊïóÔºåË´ãÁ®çÂæåÂÜçË©¶"}'
            const event = {
              type: 'conversation.item.create',
              item: {
                type: 'message',
                role: 'user',
                content: [{ type: 'input_text', text: errorText }],
              },
            }
            dataChannelRef.current.send(JSON.stringify(event))
  
            if (audioElementRef.current) {
              audioElementRef.current.muted = false
            }
            dataChannelRef.current.send(JSON.stringify({ type: 'response.create' }))
          }
        } finally {
          actionProcessingRef.current = false
          currentTranscriptRef.current.assistant = ''
          assistantMessageTimestampRef.current = null
          setConversationState('listening')
        }
      }
  
      // ------------------------------------------------------------------
      // 1) ‰ΩøÁî®ËÄÖË™ûÈü≥ËΩâÊñáÂ≠óÔºöÂØ¶ÊôÇÂ≠óÂπïÔºàdeltaÔºâ
      // ------------------------------------------------------------------
      if (type === 'conversation.item.input_audio_transcription.delta') {
        currentTranscriptRef.current.user += message.delta || ''
  
        if (!userMessageTimestampRef.current) {
          userMessageTimestampRef.current = Date.now()
        }
  
        setMessages(prev =>
          updateOrCreateTempMessage(
            prev,
            TEMP_USER_ID,
            'user',
            currentTranscriptRef.current.user,
            userMessageTimestampRef.current!,
          ),
        )
      }
  
      // ------------------------------------------------------------------
      // 2) ‰ΩøÁî®ËÄÖË™ûÈü≥ÁµêÊùüÔºöÂÆåÊàê‰∏ÄÂè•Ë©±
      //    üëâ ‰∏çÂú®ÈÄôË£°ÈùúÈü≥ÔºåÊòØÂê¶ÈùúÈü≥Áî±„ÄåÊòØÂê¶ÈÄ≤ÂÖ• TOOL PHASE„ÄçÊ±∫ÂÆö
      // ------------------------------------------------------------------
      else if (type === 'conversation.item.input_audio_transcription.completed') {
        const userText = message.transcript || currentTranscriptRef.current.user
        if (userText.trim()) {
          const finalTimestamp = userMessageTimestampRef.current || Date.now()
          const finalMessage: Message = {
            id: `user-${finalTimestamp}`,
            role: 'user',
            content: userText.trim(),
            timestamp: finalTimestamp,
          }
  
          setMessages(prev =>
            replaceTempMessage(prev, TEMP_USER_ID, finalMessage, TEMP_ASSISTANT_ID),
          )
  
          options.onMessage?.(finalMessage)
          currentTranscriptRef.current.user = ''
          userMessageTimestampRef.current = null
        }
  
        setConversationState('processing')
      }
  
      // ------------------------------------------------------------------
      // 3) AI Ë™ûÈü≥Â≠óÂπï deltaÔºàÂØ¶ÊôÇÔºâ
      //    üëâ Êñ∞ÈÇèËºØÔºöÊ™¢Ê∏¨„ÄåË´ãÁ®çÁ≠âÔºåÊàëÂπ´‰Ω†Êü•‰∏Ä‰∏ã [ÊôÇÈñì]„ÄçËß∏ÁôºÊü•Ë°®
      // ------------------------------------------------------------------
      else if (type === 'response.audio_transcript.delta') {
        const delta = message.delta || ''
        currentTranscriptRef.current.assistant += delta

        if (!assistantMessageTimestampRef.current) {
          assistantMessageTimestampRef.current = Date.now()
        }

        const currentText = currentTranscriptRef.current.assistant

        // ‚≠ê Êñ∞Ê™¢Ê∏¨ÈÇèËºØÔºöÊ™¢Ê∏¨„ÄåË´ãÁ®çÁ≠âÔºåÊàëÂπ´‰Ω†Êü•‰∏Ä‰∏ã„Äç
        const isTriggerPhrase = currentText.includes('Ë´ãÁ®çÁ≠â') && currentText.includes('Êü•‰∏Ä‰∏ã')

        if (isTriggerPhrase) {
          console.log('üéØ Detected trigger phrase in delta:', currentText)

          // ÂòóË©¶ÊèêÂèñÊôÇÈñì
          const extractedTime = extractTimeFromAIResponse(currentText)

          if (extractedTime && !actionProcessingRef.current) {
            // ‚úÖ ÊàêÂäüÊèêÂèñÂà∞ÊôÇÈñìÔºåÈÄ≤ÂÖ•Êü•Ë°®ÊµÅÁ®ã
            console.log('üéØ Time extracted, entering tool phase:', extractedTime)
            actionProcessingRef.current = true

            // ‚≠ê ‰∏çÁ´ãÂç≥ÈùúÈü≥ÔºåËÆìÁï∂ÂâçÂè•Â≠êË™™ÂÆå
            // ÈùúÈü≥ÊúÉÂú® response.audio_transcript.done ÊôÇÂü∑Ë°å

            // ‚≠ê ‰πü‰∏çÁ´ãÂç≥ cancelÔºåËÆìÁï∂ÂâçÂõûÊáâË™™ÂÆå
            // ÈòªÊ≠¢ÂæåÁ∫åÊñ∞ÁöÑÂõûÊáâÊúÉÂú® done ÊôÇËôïÁêÜ

            // ‰øùÁïôÂ∑≤Ë™™ÁöÑË©±Ôºà"Ë´ãÁ®çÁ≠âÔºåÊàëÂπ´‰Ω†Êü•‰∏Ä‰∏ã 12Èªû"ÔºâÂú® UI ‰∏ä
            const finalTimestamp = assistantMessageTimestampRef.current || Date.now()
            setMessages(prev => {
              const tempUserIndex = prev.findIndex(m => m.id === TEMP_USER_ID)
              return updateOrCreateTempMessage(
                prev,
                TEMP_ASSISTANT_ID,
                'assistant',
                currentText, // È°ØÁ§∫ÂÆåÊï¥ÁöÑËß∏ÁôºÂè•
                finalTimestamp,
                tempUserIndex,
              )
            })

            // Ê∏ÖÁ©∫ transcript Ê∫ñÂÇôÊé•Êî∂Êü•Ë°®ÁµêÊûúÁöÑÂõûÊáâ
            currentTranscriptRef.current.assistant = ''
            assistantMessageTimestampRef.current = null

            // Á´ãÂç≥ÈñãÂßãÊü•Ë°®Ôºà‰∏¶Ë°åÈÄ≤Ë°åÔºâ
            processCheckAvailability(extractedTime)

            return // ÈáçË¶ÅÔºöÈòªÊ≠¢ÂæåÁ∫åËôïÁêÜ
          }

          // Â¶ÇÊûúÈÇÑÊ≤íÊèêÂèñÂà∞ÂÆåÊï¥ÊôÇÈñìÔºåÁπºÁ∫åÁ¥ØÁ©çÊñáÊú¨
          // ÔºàÂèØËÉΩ AI ÈÇÑÂú®Ë™™ "Ë´ãÁ®çÁ≠âÔºåÊàëÂπ´‰Ω†Êü•‰∏Ä‰∏ã..."ÔºåÊôÇÈñìÈÇÑÊ≤íË™™ÂÆåÔºâ
        }

        // Â¶ÇÊûúÊ≠£Âú®ËôïÁêÜ action ÊàñÂ∑≤ÈùúÈü≥Ôºå‰∏çÊõ¥Êñ∞ UI
        if (audioElementRef.current?.muted || actionProcessingRef.current) {
          return
        }

        // ‚≠ê Ê≠£Â∏∏Â∞çË©±Ôºöstreaming Âä©ÁêÜÊñáÊú¨
        setMessages(prev => {
          const tempUserIndex = prev.findIndex(m => m.id === TEMP_USER_ID)
          return updateOrCreateTempMessage(
            prev,
            TEMP_ASSISTANT_ID,
            'assistant',
            currentTranscriptRef.current.assistant,
            assistantMessageTimestampRef.current!,
            tempUserIndex,
          )
        })
        setConversationState('responding')
      }
  
      // ------------------------------------------------------------------
      // 4) AI Ë™ûÈü≥Â≠óÂπïÂÆåÊàê
      // ------------------------------------------------------------------
      else if (type === 'response.audio_transcript.done') {
        const assistantText = message.transcript || currentTranscriptRef.current.assistant
        const trimmed = assistantText.trim()

        if (!trimmed) {
          currentTranscriptRef.current.assistant = ''
          assistantMessageTimestampRef.current = null
          setConversationState('listening')
          return
        }

        // ‚≠ê Ê™¢Êü•ÊòØÂê¶ÁÇ∫Êü•Ë°®Ëß∏ÁôºÂè•ÂâõË™™ÂÆå
        if (actionProcessingRef.current) {
          console.log('üîá Trigger sentence completed, muting audio and canceling further responses')

          // ÁèæÂú®ÈùúÈü≥ÔºàÁï∂ÂâçÂè•Â≠êÂ∑≤Á∂ìË™™ÂÆå‰∫ÜÔºâ
          if (audioElementRef.current) {
            audioElementRef.current.muted = true
          }

          // ÂèñÊ∂àÂæåÁ∫åÂõûÊáâÔºàÈÅøÂÖç AI ÁπºÁ∫åË™™Â§öÈ§òÁöÑË©±Ôºâ
          if (dataChannelRef.current?.readyState === 'open') {
            dataChannelRef.current.send(JSON.stringify({ type: 'response.cancel' }))
          }

          // Ê∏ÖÁ©∫ transcriptÔºåÁ≠âÂæÖÊü•Ë°®ÁµêÊûú
          currentTranscriptRef.current.assistant = ''
          assistantMessageTimestampRef.current = null
          setConversationState('processing')
          return
        }

        // ‚≠ê ‰øùÂ∫ïÊ™¢Ê∏¨ÔºöÂ¶ÇÊûú delta ÈöéÊÆµÊ≤íÊ™¢Ê∏¨Âà∞ÔºåÂú® done ÊôÇÊ™¢Ê∏¨
        const isTriggerPhrase = trimmed.includes('Ë´ãÁ®çÁ≠â') && trimmed.includes('Êü•‰∏Ä‰∏ã')

        if (isTriggerPhrase) {
          console.log('üéØ Detected trigger phrase in done (fallback):', trimmed)

          const extractedTime = extractTimeFromAIResponse(trimmed)

          if (extractedTime && dataChannelRef.current?.readyState === 'open') {
            console.log('üéØ Time extracted in done, entering tool phase:', extractedTime)
            actionProcessingRef.current = true

            // Á´ãÂç≥ÈùúÈü≥‰∏¶ cancel
            if (audioElementRef.current) {
              audioElementRef.current.muted = true
            }

            dataChannelRef.current.send(JSON.stringify({ type: 'response.cancel' }))

            // Ê∏ÖÁêÜ UI ‰∏≠ÁöÑËá®ÊôÇÊ∂àÊÅØÔºàÂ¶ÇÊûúÊúâÁöÑË©±Ôºâ
            setMessages(prev => prev.filter(m => m.id !== TEMP_ASSISTANT_ID))

            processCheckAvailability(extractedTime)

            currentTranscriptRef.current.assistant = ''
            assistantMessageTimestampRef.current = null
            setConversationState('processing')
            return
          }
        }

        // ‚≠ê Ê≠£Â∏∏Â∞çË©±ÔºöÂÆåÊàêÊúÄÁµÇÊ∂àÊÅØ
        const finalTimestamp = assistantMessageTimestampRef.current || Date.now()
        const finalMessage: Message = {
          id: `assistant-${finalTimestamp}`,
          role: 'assistant',
          content: trimmed,
          timestamp: finalTimestamp,
        }

        setMessages(prev => replaceTempMessage(prev, TEMP_ASSISTANT_ID, finalMessage))
        options.onMessage?.(finalMessage)

        currentTranscriptRef.current.assistant = ''
        assistantMessageTimestampRef.current = null
        setConversationState('listening')
      }
    },
    [options],
  )

  // Connect to N8N webhook and establish WebRTC
  const connect = useCallback(async () => {
    try {
      setConnectionStatus('connecting')

      console.log('üîå Connecting to webhook:', {
        url: webhookUrl,
        timestamp: new Date().toISOString()
      })

      // Fetch session token from N8N
      const response = await fetch(webhookUrl, {
        method: 'GET',
        mode: 'cors',
        headers: { 'Accept': 'application/json' }
      })

      console.log('üì° Webhook response:', {
        status: response.status,
        statusText: response.statusText,
        ok: response.ok,
        headers: {
          'content-type': response.headers.get('content-type'),
          'access-control-allow-origin': response.headers.get('access-control-allow-origin')
        }
      })

      if (!response.ok) {
        const errorText = await response.text()
        console.error('‚ùå Webhook error:', errorText)
        throw new Error(`Failed to connect to webhook: ${response.status} ${response.statusText}`)
      }

      const data = await response.text()
      const clientSecret = extractClientSecret(data)

      if (!clientSecret) {
        console.error('‚ùå Could not extract client_secret from response:', data)
        throw new Error('Could not find client_secret in response')
      }

      console.log('‚úÖ Client secret obtained successfully')

      // Set up WebRTC peer connection
      const peerConnection = new RTCPeerConnection()
      peerConnectionRef.current = peerConnection

      // Set up audio element for AI responses
      if (!audioElementRef.current) {
        audioElementRef.current = new Audio()
        audioElementRef.current.autoplay = true
      }

      peerConnection.ontrack = (event) => {
        if (audioElementRef.current) {
          audioElementRef.current.srcObject = event.streams[0]
        }
      }

      // Add microphone track
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      mediaStreamRef.current = stream
      stream.getTracks().forEach(track => peerConnection.addTrack(track, stream))

      // Set up data channel for transcripts
      const dataChannel = peerConnection.createDataChannel(DATA_CHANNEL_NAME)
      dataChannelRef.current = dataChannel

      dataChannel.onopen = () => {
        setConnectionStatus('connected')
        setConversationState('idle')
      }

      dataChannel.onmessage = (event) => {
        try {
          const message = JSON.parse(event.data)
          handleDataChannelMessage(message)
        } catch (error) {
          console.error('Error parsing data channel message:', error)
        }
      }

      // Create and send SDP offer
      const offer = await peerConnection.createOffer()
      await peerConnection.setLocalDescription(offer)

      const sdpResponse = await fetch(
        `https://api.openai.com/v1/realtime?model=${OPENAI_MODEL}`,
        {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${clientSecret}`,
            'Content-Type': 'application/sdp'
          },
          body: offer.sdp
        }
      )

      if (!sdpResponse.ok) {
        throw new Error(`Failed to establish WebRTC: ${sdpResponse.status}`)
      }

      const answerSdp = await sdpResponse.text()
      await peerConnection.setRemoteDescription({ type: 'answer', sdp: answerSdp })

      setIsAudioEnabled(true)
      setConversationState('listening')

    } catch (error) {
      console.error('Connection error:', error)
      setConnectionStatus('error')
      options.onError?.(error as Error)
    }
  }, [options, webhookUrl, handleDataChannelMessage])

  // Disconnect and cleanup
  const disconnect = useCallback(() => {
    peerConnectionRef.current?.close()
    dataChannelRef.current?.close()
    mediaStreamRef.current?.getTracks().forEach(track => track.stop())

    if (audioElementRef.current) {
      audioElementRef.current.pause()
      audioElementRef.current.srcObject = null
    }

    // Reset all refs
    peerConnectionRef.current = null
    dataChannelRef.current = null
    mediaStreamRef.current = null
    currentTranscriptRef.current = { user: '', assistant: '' }
    userMessageTimestampRef.current = null
    assistantMessageTimestampRef.current = null
    actionProcessingRef.current = false

    // Reset state
    setMessages([])
    setConnectionStatus('disconnected')
    setConversationState('idle')
    setIsAudioEnabled(false)
  }, [])

  // Toggle listening
  const startListening = useCallback(async () => {
    setIsAudioEnabled(true)
    setConversationState('listening')
  }, [])

  const stopListening = useCallback(() => {
    mediaStreamRef.current?.getAudioTracks().forEach(track => {
      track.enabled = false
    })
    setIsAudioEnabled(false)
    setConversationState('idle')
  }, [])

  // Send text message
  const sendMessage = useCallback((content: string) => {
    const message: Message = {
      id: `text-${Date.now()}`,
      role: 'user',
      content,
      timestamp: Date.now()
    }

    setMessages(prev => [...prev, message])
    options.onMessage?.(message)

    // Send through data channel if available
    if (dataChannelRef.current?.readyState === 'open') {
      try {
        dataChannelRef.current.send(JSON.stringify({
          type: 'conversation.item.create',
          item: {
            type: 'message',
            role: 'user',
            content: [{ type: 'input_text', text: content }]
          }
        }))

        dataChannelRef.current.send(JSON.stringify({ type: 'response.create' }))
      } catch (error) {
        console.error('Error sending message:', error)
      }
    }
  }, [options])

  const clearMessages = useCallback(() => {
    setMessages([])
  }, [])

  // Cleanup on unmount
  useEffect(() => {
    return () => disconnect()
  }, [disconnect])

  return {
    messages,
    connectionStatus,
    conversationState,
    isAudioEnabled,
    connect,
    disconnect,
    startListening,
    stopListening,
    sendMessage,
    clearMessages
  }
}
