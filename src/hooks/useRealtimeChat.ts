import { useState, useCallback, useRef, useEffect } from 'react'
import type { Message, ConnectionStatus, ConversationState, CheckReservationResponse } from '@/types'

interface UseRealtimeChatOptions {
  apiKey?: string
  webhookUrl?: string
  onMessage?: (message: Message) => void
  onError?: (error: Error) => void
}

// Constants
const TEMP_USER_ID = 'temp-user-transcript'
const TEMP_ASSISTANT_ID = 'temp-assistant-transcript'
const OPENAI_MODEL = 'gpt-4o-realtime-preview-2024-12-17'
const DATA_CHANNEL_NAME = 'oai-events'
const CHECK_RESV_URL = 'https://ici.zeabur.app/webhook/checkResv'

// Helper: Extract client secret from various response formats
function extractClientSecret(data: string): string | null {
  // Try JSON parsing first
  try {
    const jsonData = JSON.parse(data)

    // Array format (full OpenAI response from N8N)
    if (Array.isArray(jsonData) && jsonData.length > 0) {
      return jsonData[0]?.client_secret?.value || null
    }

    // Object format with nested value
    if (jsonData.client_secret) {
      if (typeof jsonData.client_secret === 'string') {
        return jsonData.client_secret
      }
      if (jsonData.client_secret.value) {
        return jsonData.client_secret.value
      }
    }

    // Alternative property names
    return jsonData.clientSecret || jsonData.data?.client_secret || null
  } catch {
    // Not JSON, try regex extraction from HTML
    const patterns = [
      /EPHEMERAL_KEY\s*=\s*["']([^"']+)["']/,
      /clientSecret\s*=\s*["']([^"']+)["']/,
      /client_secret["']?\s*:\s*["']([^"']+)["']/,
      /client_secret["']?\s*=\s*["']([^"']+)["']/
    ]

    for (const pattern of patterns) {
      const match = data.match(pattern)
      if (match?.[1]) return match[1]
    }

    return null
  }
}

// Helper: Update or create temporary message
function updateOrCreateTempMessage(
  messages: Message[],
  tempId: string,
  role: 'user' | 'assistant',
  content: string,
  timestamp: number,
  insertAfterIndex?: number
): Message[] {
  const existingIndex = messages.findIndex(m => m.id === tempId)

  if (existingIndex !== -1) {
    // Update existing temp message
    const updated = [...messages]
    updated[existingIndex] = {
      ...updated[existingIndex],
      content,
      timestamp
    }
    return updated
  }

  // Create new temp message
  const newMessage: Message = {
    id: tempId,
    role,
    content,
    timestamp
  }

  // Insert at specific position or add to end
  if (insertAfterIndex !== undefined && insertAfterIndex !== -1) {
    const updated = [...messages]
    updated.splice(insertAfterIndex + 1, 0, newMessage)
    return updated
  }

  return [...messages, newMessage]
}

// Helper: Replace temp message with final one
function replaceTempMessage(
  messages: Message[],
  tempId: string,
  finalMessage: Message,
  ensureBeforeTempId?: string
): Message[] {
  const tempIndex = messages.findIndex(m => m.id === tempId)
  const otherTempIndex = ensureBeforeTempId
    ? messages.findIndex(m => m.id === ensureBeforeTempId)
    : -1

  if (tempIndex !== -1) {
    const updated = [...messages]
    updated[tempIndex] = finalMessage

    // Ensure correct ordering if needed
    if (otherTempIndex !== -1 && otherTempIndex < tempIndex) {
      const otherMsg = updated[otherTempIndex]
      updated.splice(otherTempIndex, 1)
      const newIndex = updated.findIndex(m => m.id === finalMessage.id)
      updated.splice(newIndex + 1, 0, otherMsg)
    }

    return updated
  }

  // No temp message found - insert before other temp if exists
  if (otherTempIndex !== -1) {
    const updated = [...messages]
    updated.splice(otherTempIndex, 0, finalMessage)
    return updated
  }

  // Otherwise add to end
  return [...messages, finalMessage]
}

// Helper: Extract time from AI's response
function extractTimeFromAIResponse(text: string): string | null {
  // Special cases first
  if (text.includes('ä¸­åˆ')) {
    return '12:00'
  }

  // Pattern: "è«‹ç¨ç­‰ï¼Œæˆ‘å¹«ä½ æŸ¥ä¸€ä¸‹ [æ™‚é–“]"
  const patterns = [
    // ä¸­æ–‡æ™‚é–“æ ¼å¼ - æ¨™æº–é»æ•¸
    { regex: /æŸ¥ä¸€ä¸‹\s*(\d{1,2})\s*é»\s*åŠ/, handler: (m: RegExpMatchArray) => {
      const hour = parseInt(m[1])
      return `${hour.toString().padStart(2, '0')}:30`
    }},
    { regex: /æŸ¥ä¸€ä¸‹\s*(\d{1,2})\s*é»\s*(\d{1,2})\s*åˆ†/, handler: (m: RegExpMatchArray) => {
      const hour = parseInt(m[1])
      const minute = parseInt(m[2])
      return `${hour.toString().padStart(2, '0')}:${minute.toString().padStart(2, '0')}`
    }},
    { regex: /æŸ¥ä¸€ä¸‹\s*(\d{1,2})\s*é»(?!\s*åŠ|\s*\d)/, handler: (m: RegExpMatchArray) => {
      const hour = parseInt(m[1])
      return `${hour.toString().padStart(2, '0')}:00`
    }},

    // æ—©ä¸Š/ä¸‹åˆ/æ™šä¸Š
    { regex: /æŸ¥ä¸€ä¸‹\s*æ—©ä¸Š\s*(\d{1,2})\s*é»/, handler: (m: RegExpMatchArray) => {
      let hour = parseInt(m[1])
      if (hour === 12) hour = 0
      return `${hour.toString().padStart(2, '0')}:00`
    }},
    { regex: /æŸ¥ä¸€ä¸‹\s*ä¸‹åˆ\s*(\d{1,2})\s*é»/, handler: (m: RegExpMatchArray) => {
      let hour = parseInt(m[1])
      if (hour < 12) hour += 12
      return `${hour.toString().padStart(2, '0')}:00`
    }},
    { regex: /æŸ¥ä¸€ä¸‹\s*æ™šä¸Š\s*(\d{1,2})\s*é»/, handler: (m: RegExpMatchArray) => {
      let hour = parseInt(m[1])
      if (hour < 12) hour += 12
      return `${hour.toString().padStart(2, '0')}:00`
    }},

    // 24å°æ™‚åˆ¶æ ¼å¼
    { regex: /æŸ¥ä¸€ä¸‹\s*(\d{1,2}):(\d{2})/, handler: (m: RegExpMatchArray) => {
      const hour = parseInt(m[1])
      const minute = parseInt(m[2])
      return `${hour.toString().padStart(2, '0')}:${minute.toString().padStart(2, '0')}`
    }},

    // PM/AM æ ¼å¼
    { regex: /æŸ¥ä¸€ä¸‹\s*(\d{1,2})\s*pm/i, handler: (m: RegExpMatchArray) => {
      let hour = parseInt(m[1])
      if (hour < 12) hour += 12
      return `${hour.toString().padStart(2, '0')}:00`
    }},
    { regex: /æŸ¥ä¸€ä¸‹\s*(\d{1,2})\s*am/i, handler: (m: RegExpMatchArray) => {
      let hour = parseInt(m[1])
      if (hour === 12) hour = 0
      return `${hour.toString().padStart(2, '0')}:00`
    }},
  ]

  // Try each pattern
  for (const { regex, handler } of patterns) {
    const match = text.match(regex)
    if (match) {
      try {
        return handler(match)
      } catch (error) {
        console.error('âŒ æ™‚é–“æå–éŒ¯èª¤:', error)
        continue
      }
    }
  }

  return null
}

// Helper: Call reservation check API
async function checkReservationAvailability(time: string): Promise<CheckReservationResponse> {
  try {
    const response = await fetch(CHECK_RESV_URL, {
      method: 'POST',
      mode: 'cors',
      headers: {
        'Content-Type': 'application/json',
        'Accept': 'application/json'
      },
      body: JSON.stringify({ time })
    })

    if (!response.ok) {
      console.error('âŒ API éŒ¯èª¤:', response.status, response.statusText)
      throw new Error(`API request failed: ${response.status}`)
    }

    const data = await response.json()
    const availabilityValue = data.Availability ?? data.availability ?? data.available ?? false

    return {
      available: availabilityValue,
      time: data.bookTime ?? time,
      message: data.message
    }
  } catch (error) {
    console.error('âŒ æŸ¥è©¢å¤±æ•—:', error instanceof Error ? error.message : String(error))
    throw error
  }
}

export function useRealtimeChat(options: UseRealtimeChatOptions = {}) {
  const [messages, setMessages] = useState<Message[]>([])
  const [connectionStatus, setConnectionStatus] = useState<ConnectionStatus>('disconnected')
  const [conversationState, setConversationState] = useState<ConversationState>('idle')
  const [isAudioEnabled, setIsAudioEnabled] = useState(false)

  const peerConnectionRef = useRef<RTCPeerConnection | null>(null)
  const dataChannelRef = useRef<RTCDataChannel | null>(null)
  const audioElementRef = useRef<HTMLAudioElement | null>(null)
  const mediaStreamRef = useRef<MediaStream | null>(null)
  const currentTranscriptRef = useRef({ user: '', assistant: '' })
  const userMessageTimestampRef = useRef<number | null>(null)
  const assistantMessageTimestampRef = useRef<number | null>(null)
  const actionProcessingRef = useRef<boolean>(false)

  const greetedRef = useRef(false)

  // Response ID tracking - ç”¨æ–¼éæ¿¾èˆŠçš„éŸ³é »
  const currentResponseIdRef = useRef<string | null>(null)
  const waitingForNewResponseRef = useRef<boolean>(false)

  const webhookUrl = options.webhookUrl ||
    import.meta.env.VITE_WEBHOOK_URL ||
    'https://ici.zeabur.app/webhook/realtime-ai'

  // Handle data channel messages
  const handleDataChannelMessage = useCallback(
    async (message: any) => {
      const { type } = message

      // ---- æ‰“æ–·æ¨¡å‹ï¼šåœæ­¢ç•¶å‰å›æ‡‰ + æ¸…ç©ºéŸ³é »ç·©è¡å€ ----
      const interruptModel = (reason?: string) => {
        if (!dataChannelRef.current || dataChannelRef.current.readyState !== 'open') return

        console.log('ğŸ›‘ æ‰“æ–·æ¨¡å‹', reason || '')

        // åœæ‰ç•¶å‰æ­£åœ¨ç”Ÿæˆçš„å›æ‡‰
        dataChannelRef.current.send(JSON.stringify({ type: 'response.cancel' }))

        // æ¸…æ‰å·²ç¶“ç·©æ²–ä½†é‚„æ²’æ’­å®Œçš„éŸ³é »
        dataChannelRef.current.send(JSON.stringify({ type: 'output_audio_buffer.clear' }))

        // å¯é¸ï¼šä¹Ÿé †ä¾¿æŠŠæœ¬åœ°å­—å¹•ç‹€æ…‹æ¸…ä¸€ä¸‹
        currentTranscriptRef.current.assistant = ''
        assistantMessageTimestampRef.current = null
      }

      // ---- å°å·¥å…·ï¼šå°è£ä¸€æ¬¡è™•ç† CHECK_AVAILABILITY çš„æµç¨‹ ----
      const processCheckAvailability = async (time: string) => {
        try {
          const result = await checkReservationAvailability(time)
          console.log('âœ… æŸ¥è©¢çµæœ:', time, result.available ? 'ç„¡ç©ºä½' : 'æœ‰ç©ºä½')

          if (dataChannelRef.current?.readyState === 'open') {
            const payload = {
              bookTime: result.time,
              Availability: result.available, // false=æœ‰ä½, true=æ²’ä½ï¼ˆæŒ‰ prompts å®šç¾©ï¼‰
            }
            const text = `AVAILABILITY_RESULT: ${JSON.stringify(payload)}`

            const event = {
              type: 'conversation.item.create',
              item: {
                type: 'message',
                role: 'user',
                content: [{ type: 'input_text', text }],
              },
            }

            // æ¸…ç©º AI çš„éŸ³é »éšŠåˆ—ï¼Œé¿å…æ’­æ”¾èˆŠçš„éŸ³é »ç·©å­˜
            interruptModel('æŸ¥è¡¨å®Œæˆï¼Œæº–å‚™ç™¼é€çµæœ')
            await new Promise(resolve => setTimeout(resolve, 100))

            // ç™¼é€æŸ¥è¡¨çµæœçµ¦ AI
            dataChannelRef.current.send(JSON.stringify(event))

            // â­ é—œéµæ”¹é€²ï¼šè§¸ç™¼æ–°å›æ‡‰ï¼Œä½†ä¸ç«‹å³è§£é™¤éœéŸ³
            // ç­‰å¾… response.created äº‹ä»¶ï¼ˆè¡¨ç¤ºæ–°å›æ‡‰é–‹å§‹ï¼‰æ‰è§£é™¤éœéŸ³
            // é€™æ¨£å¯ä»¥ç¢ºä¿æ’­æ”¾çš„æ˜¯æ–°å›æ‡‰çš„éŸ³é »ï¼Œè€Œä¸æ˜¯èˆŠçš„ç·©å­˜éŸ³é »
            waitingForNewResponseRef.current = true
            console.log('â³ ç­‰å¾…æ–°å›æ‡‰é–‹å§‹...')

            // è§¸ç™¼æ–°çš„å›è¦†
            dataChannelRef.current.send(JSON.stringify({ type: 'response.create' }))
          }
        } catch (error) {
          console.error('âŒ æŸ¥è©¢éŒ¯èª¤:', error)

          if (dataChannelRef.current?.readyState === 'open') {
            const errorText = 'AVAILABILITY_RESULT: {"error": "æŸ¥è©¢å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦"}'
            const event = {
              type: 'conversation.item.create',
              item: {
                type: 'message',
                role: 'user',
                content: [{ type: 'input_text', text: errorText }],
              },
            }

            // æ¸…ç©ºéšŠåˆ—ä¸¦ç™¼é€éŒ¯èª¤çµæœ
            interruptModel('æŸ¥è©¢éŒ¯èª¤')
            await new Promise(resolve => setTimeout(resolve, 100))
            dataChannelRef.current.send(JSON.stringify(event))

            // éŒ¯èª¤æƒ…æ³ä¹Ÿç­‰å¾…æ–°å›æ‡‰æ‰è§£é™¤éœéŸ³
            waitingForNewResponseRef.current = true

            dataChannelRef.current.send(JSON.stringify({ type: 'response.create' }))
          }
        } finally {
          actionProcessingRef.current = false
          currentTranscriptRef.current.assistant = ''
          assistantMessageTimestampRef.current = null
          setConversationState('listening')
        }
      }

      // ------------------------------------------------------------------
      // 0) Response Created - è¨˜éŒ„ç•¶å‰ response IDï¼Œç”¨æ–¼éæ¿¾èˆŠéŸ³é »
      // ------------------------------------------------------------------
      if (type === 'response.created') {
        const responseId = message.response?.id
        if (responseId) {
          currentResponseIdRef.current = responseId
          console.log('ğŸ†• æ–°å›æ‡‰é–‹å§‹:', responseId)

          // ä¸ç®¡æ˜¯ä¸æ˜¯æŸ¥è¡¨çµæœï¼Œä¸€æ—¦æœ‰æ–°å›æ‡‰é–‹å§‹å°±ç¢ºä¿è²éŸ³æ˜¯é–‹çš„
          if (audioElementRef.current) {
            audioElementRef.current.muted = false
          }
          waitingForNewResponseRef.current = false
        }
      }

      // ------------------------------------------------------------------
      // 0.5) ä½¿ç”¨è€…é–‹å§‹èªªè©±ï¼šæ‰“æ–·ç•¶å‰ AI èªéŸ³
      // ------------------------------------------------------------------
      else if (type === 'input_audio_buffer.speech_started') {
        console.log('ğŸ—£ ç”¨æˆ¶é–‹å§‹èªªè©±ï¼Œæ‰“æ–·ç•¶å‰å›æ‡‰')
        interruptModel('ç”¨æˆ¶æ’è©±')

        // ä¸å†å¼·åˆ¶éœéŸ³ï¼Œå› ç‚º output_audio_buffer.clear å·²ç¶“æŠŠèˆŠéŸ³é »æ¸…æ‰äº†
        // å¦‚æœä½ çœŸçš„å¾ˆæ”¾å¿ƒï¼Œå¯ä»¥å…ˆæš«æ™‚éœéŸ³å†ç«‹åˆ»è§£é™¤ï¼Œä½†é€šå¸¸æ²’å¿…è¦

        // ç•¥éå¾ŒçºŒï¼Œç­‰æ–°çš„ ASR delta ä¾†æ›´æ–° user å­—å¹•å³å¯
      }

      // ------------------------------------------------------------------
      // 1) ä½¿ç”¨è€…èªéŸ³è½‰æ–‡å­—ï¼šå¯¦æ™‚å­—å¹•ï¼ˆdeltaï¼‰
      // ------------------------------------------------------------------
      else if (type === 'conversation.item.input_audio_transcription.delta') {
        currentTranscriptRef.current.user += message.delta || ''
  
        if (!userMessageTimestampRef.current) {
          userMessageTimestampRef.current = Date.now()
        }
  
        setMessages(prev =>
          updateOrCreateTempMessage(
            prev,
            TEMP_USER_ID,
            'user',
            currentTranscriptRef.current.user,
            userMessageTimestampRef.current!,
          ),
        )
      }
  
      // ------------------------------------------------------------------
      // 2) ä½¿ç”¨è€…èªéŸ³çµæŸï¼šå®Œæˆä¸€å¥è©±
      //    ğŸ‘‰ ä¸åœ¨é€™è£¡éœéŸ³ï¼Œæ˜¯å¦éœéŸ³ç”±ã€Œæ˜¯å¦é€²å…¥ TOOL PHASEã€æ±ºå®š
      // ------------------------------------------------------------------
      else if (type === 'conversation.item.input_audio_transcription.completed') {
        const userText = message.transcript || currentTranscriptRef.current.user
        if (userText.trim()) {
          const finalTimestamp = userMessageTimestampRef.current || Date.now()
          const finalMessage: Message = {
            id: `user-${finalTimestamp}`,
            role: 'user',
            content: userText.trim(),
            timestamp: finalTimestamp,
          }
  
          setMessages(prev =>
            replaceTempMessage(prev, TEMP_USER_ID, finalMessage, TEMP_ASSISTANT_ID),
          )
  
          options.onMessage?.(finalMessage)
          currentTranscriptRef.current.user = ''
          userMessageTimestampRef.current = null
        }
  
        setConversationState('processing')
      }
  
      // ------------------------------------------------------------------
      // 3) AI èªéŸ³å­—å¹• deltaï¼ˆå¯¦æ™‚ï¼‰
      // ------------------------------------------------------------------
      else if (type === 'response.audio_transcript.delta') {
        const delta = message.delta || ''
        currentTranscriptRef.current.assistant += delta

        if (!assistantMessageTimestampRef.current) {
          assistantMessageTimestampRef.current = Date.now()
        }

        // å¦‚æœæ­£åœ¨è™•ç†æŸ¥è¡¨ï¼Œä¸æ›´æ–° UI
        if (actionProcessingRef.current) {
          return
        }

        // æ­£å¸¸å°è©±ï¼šstreaming åŠ©ç†æ–‡æœ¬
        setMessages(prev => {
          const tempUserIndex = prev.findIndex(m => m.id === TEMP_USER_ID)
          return updateOrCreateTempMessage(
            prev,
            TEMP_ASSISTANT_ID,
            'assistant',
            currentTranscriptRef.current.assistant,
            assistantMessageTimestampRef.current!,
            tempUserIndex,
          )
        })
        setConversationState('responding')
      }
  
      // ------------------------------------------------------------------
      // 4) AI èªéŸ³å­—å¹•å®Œæˆ
      // ------------------------------------------------------------------
      else if (type === 'response.audio_transcript.done') {
        const assistantText = message.transcript || currentTranscriptRef.current.assistant
        const trimmed = assistantText.trim()

        if (!trimmed) {
          currentTranscriptRef.current.assistant = ''
          assistantMessageTimestampRef.current = null
          setConversationState('listening')
          return
        }

        // â­ æª¢æ¸¬æ˜¯å¦ç‚ºæŸ¥è¡¨è§¸ç™¼å¥
        const isTriggerPhrase = trimmed.includes('è«‹ç¨ç­‰') && trimmed.includes('æŸ¥ä¸€ä¸‹')

        if (isTriggerPhrase && !actionProcessingRef.current) {
          const extractedTime = extractTimeFromAIResponse(trimmed)

          if (extractedTime && dataChannelRef.current?.readyState === 'open') {
            console.log('ğŸ” æŸ¥è¡¨è«‹æ±‚:', extractedTime)
            actionProcessingRef.current = true

            // ä¸å†å¼·åˆ¶éœéŸ³ï¼Œå› ç‚º output_audio_buffer.clear å·²ç¶“æŠŠèˆŠéŸ³é »é »æ¸…æ‰äº†
            // å¦‚æœä½ çœŸçš„å¾ˆæ”¾å¿ƒï¼Œå¯ä»¥å…ˆæš«æ™‚éœéŸ³å†ç«‹åˆ»è§£é™¤ï¼Œä½†é€šå¸¸æ²’å¿…è¦

            interruptModel('è§¸ç™¼æŸ¥è¡¨')

            // ä¿ç•™è§¸ç™¼å¥åœ¨ UI ä¸Š
            const finalTimestamp = assistantMessageTimestampRef.current || Date.now()
            const finalMessage: Message = {
              id: `assistant-${finalTimestamp}`,
              role: 'assistant',
              content: trimmed,
              timestamp: finalTimestamp,
            }
            setMessages(prev => replaceTempMessage(prev, TEMP_ASSISTANT_ID, finalMessage))

            // é–‹å§‹æŸ¥è¡¨
            processCheckAvailability(extractedTime)

            currentTranscriptRef.current.assistant = ''
            assistantMessageTimestampRef.current = null
            setConversationState('processing')
            return
          }
        }

        // â­ æ­£å¸¸å°è©±ï¼šå®Œæˆæœ€çµ‚æ¶ˆæ¯
        const finalTimestamp = assistantMessageTimestampRef.current || Date.now()
        const finalMessage: Message = {
          id: `assistant-${finalTimestamp}`,
          role: 'assistant',
          content: trimmed,
          timestamp: finalTimestamp,
        }

        setMessages(prev => replaceTempMessage(prev, TEMP_ASSISTANT_ID, finalMessage))
        options.onMessage?.(finalMessage)

        currentTranscriptRef.current.assistant = ''
        assistantMessageTimestampRef.current = null
        setConversationState('listening')
      }
    },
    [options],
  )

  // Connect to N8N webhook and establish WebRTC
  const connect = useCallback(async () => {
    try {
      setConnectionStatus('connecting')

      console.log('ğŸ”Œ Connecting to webhook:', {
        url: webhookUrl,
        timestamp: new Date().toISOString()
      })

      // Fetch session token from N8N
      const response = await fetch(webhookUrl, {
        method: 'GET',
        mode: 'cors',
        headers: { 'Accept': 'application/json' }
      })

      console.log('ğŸ“¡ Webhook response:', {
        status: response.status,
        statusText: response.statusText,
        ok: response.ok,
        headers: {
          'content-type': response.headers.get('content-type'),
          'access-control-allow-origin': response.headers.get('access-control-allow-origin')
        }
      })

      if (!response.ok) {
        const errorText = await response.text()
        console.error('âŒ Webhook error:', errorText)
        throw new Error(`Failed to connect to webhook: ${response.status} ${response.statusText}`)
      }

      const data = await response.text()
      const clientSecret = extractClientSecret(data)

      if (!clientSecret) {
        console.error('âŒ Could not extract client_secret from response:', data)
        throw new Error('Could not find client_secret in response')
      }

      console.log('âœ… Client secret obtained successfully')

      // Set up WebRTC peer connection
      const peerConnection = new RTCPeerConnection()
      peerConnectionRef.current = peerConnection

      // Set up audio element for AI responses
      if (!audioElementRef.current) {
        audioElementRef.current = new Audio()
        audioElementRef.current.autoplay = true
      }

      peerConnection.ontrack = (event) => {
        if (audioElementRef.current) {
          audioElementRef.current.srcObject = event.streams[0]
        }
      }

      // Add microphone track
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      mediaStreamRef.current = stream
      stream.getTracks().forEach(track => peerConnection.addTrack(track, stream))

      // Set up data channel for transcripts
      const dataChannel = peerConnection.createDataChannel(DATA_CHANNEL_NAME)
      dataChannelRef.current = dataChannel



      dataChannel.onopen = () => {
        setConnectionStatus('connected')
        setConversationState('idle')
      
        if (!greetedRef.current) {
          greetedRef.current = true
          dataChannel.send(JSON.stringify({
            type: 'response.create',
            response: {
              instructions: 'è«‹ç›´æ¥ä»¥è‡ªç„¶èªæ°£èªªï¼šã€Œä½ å¥½ï¼Œæ­¡è¿æ‚¨è‡´é›»å®šè¬™é…’é¤¨è¨‚ä½å°ˆç·šï¼Œè«‹å•æ‚¨æƒ³è¨‚å¹¾é»çš„ä½å­å‘¢ï¼Ÿã€åªèªªé€™ä¸€å¥ï¼Œèªªå®Œå¾Œå®‰éœç­‰å¾…å°æ–¹å›ç­”ï¼Œä¸è¦å¤šèªªå…¶ä»–å…§å®¹ã€‚'
            }
          }))
          setConversationState('responding')
        }
      }

      dataChannel.onmessage = (event) => {
        try {
          const message = JSON.parse(event.data)
          handleDataChannelMessage(message)
        } catch (error) {
          console.error('Error parsing data channel message:', error)
        }
      }

      // Create and send SDP offer
      const offer = await peerConnection.createOffer()
      await peerConnection.setLocalDescription(offer)

      const sdpResponse = await fetch(
        `https://api.openai.com/v1/realtime?model=${OPENAI_MODEL}`,
        {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${clientSecret}`,
            'Content-Type': 'application/sdp'
          },
          body: offer.sdp
        }
      )

      if (!sdpResponse.ok) {
        throw new Error(`Failed to establish WebRTC: ${sdpResponse.status}`)
      }

      const answerSdp = await sdpResponse.text()
      await peerConnection.setRemoteDescription({ type: 'answer', sdp: answerSdp })

      setIsAudioEnabled(true)
      setConversationState('listening')

    } catch (error) {
      console.error('Connection error:', error)
      setConnectionStatus('error')
      options.onError?.(error as Error)
    }
  }, [options, webhookUrl, handleDataChannelMessage])

  // Disconnect and cleanup
  const disconnect = useCallback(() => {
    peerConnectionRef.current?.close()
    dataChannelRef.current?.close()
    mediaStreamRef.current?.getTracks().forEach(track => track.stop())

    if (audioElementRef.current) {
      audioElementRef.current.pause()
      audioElementRef.current.srcObject = null
    }

    // Reset all refs
    peerConnectionRef.current = null
    dataChannelRef.current = null
    mediaStreamRef.current = null
    currentTranscriptRef.current = { user: '', assistant: '' }
    userMessageTimestampRef.current = null
    assistantMessageTimestampRef.current = null
    actionProcessingRef.current = false

    // Reset state
    setMessages([])
    setConnectionStatus('disconnected')
    setConversationState('idle')
    setIsAudioEnabled(false)
  }, [])

  // Toggle listening
  const startListening = useCallback(async () => {
    setIsAudioEnabled(true)
    setConversationState('listening')
  }, [])

  const stopListening = useCallback(() => {
    mediaStreamRef.current?.getAudioTracks().forEach(track => {
      track.enabled = false
    })
    setIsAudioEnabled(false)
    setConversationState('idle')
  }, [])

  // Send text message
  const sendMessage = useCallback((content: string) => {
    const message: Message = {
      id: `text-${Date.now()}`,
      role: 'user',
      content,
      timestamp: Date.now()
    }

    setMessages(prev => [...prev, message])
    options.onMessage?.(message)

    // Send through data channel if available
    if (dataChannelRef.current?.readyState === 'open') {
      try {
        dataChannelRef.current.send(JSON.stringify({
          type: 'conversation.item.create',
          item: {
            type: 'message',
            role: 'user',
            content: [{ type: 'input_text', text: content }]
          }
        }))

        dataChannelRef.current.send(JSON.stringify({ type: 'response.create' }))
      } catch (error) {
        console.error('Error sending message:', error)
      }
    }
  }, [options])

  const clearMessages = useCallback(() => {
    setMessages([])
  }, [])

  // Cleanup on unmount
  useEffect(() => {
    return () => disconnect()
  }, [disconnect])

  return {
    messages,
    connectionStatus,
    conversationState,
    isAudioEnabled,
    connect,
    disconnect,
    startListening,
    stopListening,
    sendMessage,
    clearMessages
  }
}
